# Predicting the Number of Hits of Online Listings in the German Housing Market


### General Information 

This research project that should allow us to gain first hand experience with the methods, challenges and possibilities of the machine learning pipeline. In order to achieve this, we will implement a prediction task with a number of different model architectures, hyper-parameters and methods. Correspondingly, this project falls under categories 1 and 5 of the project types described in the document titled \textit{Guidance Notes for Final Projects}: applying an existing ML model to a new task or problem and experimental and/or theoretical analysis of machine learning models. From a theoretical perspective, our project falls in the supervised learning paradigm

The project's main use case will be predicting the lifespan of online listings in the German housing market. Specifically, we will use listing features from a large dataset to predict how long these housing offers stay online. The dataset in question includes a good number of interesting listing features which will allow us to explore ancillary questions that should allow us to explore how changing specific parameters in our models produce different performance results. Just to mention an example, we will try using balanced and unbalanced training sets to explore how these different approaches affect different performance metrics.

### Data

We will be using a dataset called _Real estate data for Germany (RWI-GEO-RED)_, described in Breidenbach and Schaffner's paper (https://www.degruyter.com/document/doi/10.1515/ger-2019-0126). The dataset consists of 87 variables describing residential commercial real estate listings from ImmobilienScout24 (www.immobilienscout24.de), considered the largest online platform for real estate offers in Germany. These variables describe not only the property being listed (e.g. surface area, number of rooms, number of bathrooms, etc.) but also contains information about the ad itself (e.g. days of availability, number of hits, etc). Table 1 provides a glimpse into the variables contained in the dataset and their corresponding sample size.

In order to obtain access to the dataset, we must submit a formal request to the dataset managers. We are currently working on it and expect to send the request in the coming days. If for some unforeseeable reason, we are unable to access the proposed data set, there is an alternative dataset we can use available here in Kaggle: (https://www.kaggle.com/corrieaar/apartment-rental-offers-in-germany). We reckon a Kaggle dataset is not the best and most professional option, but we would only use this as a plan B.

With respect to the project's need for specialized hardware -such as access to a GPU farm or the like- or computational tools -such as cloud computing beyond Google Workspace-, we are at this point in time unaware of any particular requirements. However, we believe it is a good idea to become acquainted with services such as DataBricks or Amazon Web Services, so processing our project with the free versions of one of these services could become part of the learning objectives if time permits.

### Contributors

- [Carlo Gre√ü](https://github.com/carlo-gress)
- [Wojciech Kuznicki](https://github.com/wkuznicki)
- [Santiago Sordo Ruz](https://github.com/odros)

